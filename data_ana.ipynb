{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import asc, col, isnan, when, count, median, udf, concat, month, year, substring, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import duckdb\n",
    "import pyarrow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('data_ana.db')\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"data_ana\") \\\n",
    "    .set(\"spark.driver.memory\", \"12g\")\\\n",
    "    .set(\"spark.executor.cores\",\"8\") \\\n",
    "    .set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[fecha_dato: date, ncodpers: double, ind_empleado: string, pais_residencia: string, sexo: string, age: string, fecha_alta: date, ind_nuevo: string, antiguedad: string, indrel: string, ult_fec_cli_1t: date, indrel_1mes: string, tiprel_1mes: string, indresi: string, indext: string, conyuemp: string, canal_entrada: string, indfall: string, tipodom: string, cod_prov: string, nomprov: string, ind_actividad_cliente: string, renta: double, segmento: string, ind_ahor_fin_ult1: int, ind_aval_fin_ult1: int, ind_cco_fin_ult1: int, ind_cder_fin_ult1: int, ind_cno_fin_ult1: int, ind_ctju_fin_ult1: int, ind_ctma_fin_ult1: int, ind_ctop_fin_ult1: int, ind_ctpp_fin_ult1: int, ind_deco_fin_ult1: int, ind_deme_fin_ult1: int, ind_dela_fin_ult1: int, ind_ecue_fin_ult1: int, ind_fond_fin_ult1: int, ind_hip_fin_ult1: int, ind_plan_fin_ult1: int, ind_pres_fin_ult1: int, ind_reca_fin_ult1: int, ind_tjcr_fin_ult1: int, ind_valo_fin_ult1: int, ind_viv_fin_ult1: int, ind_nomina_ult1: string, ind_nom_pens_ult1: string, ind_recibo_ult1: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path =\"/workspace/data.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.orderBy(asc(\"fecha_dato\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ncodpers\", df[\"ncodpers\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"ind_nomina_ult1\", df[\"ind_nomina_ult1\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"ind_nom_pens_ult1\", df[\"ind_nom_pens_ult1\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_list = [ \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\",\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\",\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\",\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\",\n",
    "    \"ind_nomina_ult1\",\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\"]\n",
    "\n",
    "feature_cols = [F.col(col) for col in feature_list]\n",
    "features_array = F.array(*feature_cols).alias(\"features\")\n",
    "\n",
    "grouped_df = df.select(\"ncodpers\", *feature_list).withColumn(\"features\", features_array) \\\n",
    "    .groupBy(\"ncodpers\") \\\n",
    "    .agg(F.collect_list(\"features\").alias(\"features_list\"))\n",
    "\n",
    "customer_feature = {row[\"ncodpers\"]: row[\"features_list\"] for row in grouped_df.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {feature: 0 for feature in feature_list}\n",
    "\n",
    "for features in customer_feature.values():\n",
    "    for feat_array in features:\n",
    "        for idx, val in enumerate(feat_array):\n",
    "            counts[feature_list[idx]] += 1 if val == 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occur_times = spark.createDataFrame(list(counts.items()), [\"feature\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_occur_times = feature_occur_times.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserException",
     "evalue": "Parser Error: syntax error at or near \"DATABASE\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE DATABASE ana_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m conn\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE feature_occur_times AS SELECT * FROM feature_occur_times\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mParserException\u001b[0m: Parser Error: syntax error at or near \"DATABASE\""
     ]
    }
   ],
   "source": [
    "conn.sql(\"CREATE TABLE feature_occur_times AS SELECT * FROM feature_occur_times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
