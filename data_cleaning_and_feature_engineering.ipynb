{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d327cf-4323-48c1-8583-184b3fcb93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import asc, col, isnan, when, count, median, udf, concat, month, year, substring, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe265f6-bab5-4c95-9d0d-042def8a922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/23 14:46:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[fecha_dato: date, ncodpers: double, ind_empleado: string, pais_residencia: string, sexo: string, age: string, fecha_alta: date, ind_nuevo: string, antiguedad: string, indrel: string, ult_fec_cli_1t: date, indrel_1mes: string, tiprel_1mes: string, indresi: string, indext: string, conyuemp: string, canal_entrada: string, indfall: string, tipodom: string, cod_prov: string, nomprov: string, ind_actividad_cliente: string, renta: double, segmento: string, ind_ahor_fin_ult1: int, ind_aval_fin_ult1: int, ind_cco_fin_ult1: int, ind_cder_fin_ult1: int, ind_cno_fin_ult1: int, ind_ctju_fin_ult1: int, ind_ctma_fin_ult1: int, ind_ctop_fin_ult1: int, ind_ctpp_fin_ult1: int, ind_deco_fin_ult1: int, ind_deme_fin_ult1: int, ind_dela_fin_ult1: int, ind_ecue_fin_ult1: int, ind_fond_fin_ult1: int, ind_hip_fin_ult1: int, ind_plan_fin_ult1: int, ind_pres_fin_ult1: int, ind_reca_fin_ult1: int, ind_tjcr_fin_ult1: int, ind_valo_fin_ult1: int, ind_viv_fin_ult1: int, ind_nomina_ult1: string, ind_nom_pens_ult1: string, ind_recibo_ult1: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a spark session and read the data\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"data_cleaning\") \\\n",
    "    .set(\"spark.driver.memory\", \"15g\")\\\n",
    "    .set(\"spark.executor.cores\",\"8\") \\\n",
    "    .set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "csv_file_path =\"/workspace/data.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.orderBy(asc(\"fecha_dato\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154bd5",
   "metadata": {},
   "source": [
    "# Xử lý cột \"age\"\n",
    "\n",
    "Đoạn mã này làm sạch và gán giá trị cho cột \"age\" trong DataFrame PySpark, bao gồm:\n",
    "\n",
    "1.  **Tính tuổi trung bình:**\n",
    "    *   `mean_age_18_to_30`: Trung bình tuổi từ 18 đến 30 (bao gồm cả hai đầu).\n",
    "    *   `mean_age_30_to_100`: Trung bình tuổi từ 30 đến 100 (bao gồm cả hai đầu).\n",
    "    *   `overall_mean_age`: Trung bình tuổi của toàn bộ DataFrame.\n",
    "\n",
    "    Sử dụng `df.filter()`, `F.col()`, `F.mean()`, và `.collect()[0][0]` để lấy giá trị.  Lưu ý: `.collect()` đưa dữ liệu về driver, cẩn thận với DataFrame lớn.\n",
    "\n",
    "2.  **Xử lý ngoại lai:**\n",
    "    *   Thay thế tuổi `< 18` bằng `mean_age_18_to_30`.\n",
    "    *   Thay thế tuổi `> 100` bằng `mean_age_30_to_100`.\n",
    "    *   Giữ nguyên tuổi từ 18-100.\n",
    "\n",
    "    Dùng `df.withColumn()`, `F.when()`, và `.otherwise()`.\n",
    "\n",
    "3.  **Điền giá trị thiếu (NA/null):**\n",
    "    *   Điền giá trị thiếu trong cột \"age\" bằng `overall_mean_age`.\n",
    "    *   Sử dụng `df.fillna()`.\n",
    "\n",
    "4.  **Chuyển kiểu dữ liệu:**\n",
    "    *   Chuyển cột \"age\" sang kiểu số nguyên (int).\n",
    "    *    Dùng `F.col(\"age\").cast(\"int\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0beb8b49-ecd7-4b20-a49b-4d70d7d384b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Fill NA with overall mean\n",
    "mean_age_18_to_30 = df.filter((F.col(\"age\") >= 18) & (F.col(\"age\") <= 30)).select(F.mean(\"age\")).collect()[0][0]\n",
    "mean_age_30_to_100 = df.filter((F.col(\"age\") >= 30) & (F.col(\"age\") <= 100)).select(F.mean(\"age\")).collect()[0][0]\n",
    "overall_mean_age = df.select(F.mean(\"age\")).collect()[0][0]\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"age\",\n",
    "    F.when(F.col(\"age\") < 18, mean_age_18_to_30)\n",
    "     .when(F.col(\"age\") > 100, mean_age_30_to_100)\n",
    "     .otherwise(F.col(\"age\"))\n",
    ")\n",
    "\n",
    "df = df.fillna({\"age\": overall_mean_age})\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"int\"))\n",
    "\n",
    "# df = df.withColumn(\n",
    "#     \"age\",\n",
    "#     F.when(F.col(\"age\").isNull(), overall_mean_age).otherwise(F.col(\"age\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309242a",
   "metadata": {},
   "source": [
    "# Điền giá trị thiếu (Missing Value Imputation)\n",
    "\n",
    "Đoạn mã này thực hiện điền các giá trị thiếu (NA/null) trong các cột khác nhau của một DataFrame PySpark (`df`). Mỗi dòng sử dụng `df.fillna()` để điền giá trị thiếu trong một cột cụ thể bằng một giá trị cố định(trong trường hợp này là giá trị xuất hiện nhiều nhất)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1492a6-7533-4887-8575-f85a76055047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filling missing falue with the most common status\n",
    "df = df.fillna({\"ind_nuevo\": 1})\n",
    "df = df.fillna({\"indrel\": 1})\n",
    "df = df.fillna({\"indfall\": \"N\"})\n",
    "df = df.fillna({\"tiprel_1mes\": \"A\"})\n",
    "#Fill ult_fec_cli_1t with a value in the future to indicate that they are still a primary customer\n",
    "df = df.fillna({\"ult_fec_cli_1t\" : '2020-01-01'})\n",
    "df = df.fillna({\"conyuemp\" : \"-1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f306a",
   "metadata": {},
   "source": [
    "# Chuyển đổi kiểu dữ liệu sang số nguyên (IntegerType) trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện chuyển đổi kiểu dữ liệu của một danh sách các cột trong DataFrame (`df`) sang kiểu số nguyên (`IntegerType`).  Điều này thường cần thiết sau khi thực hiện các thao tác như điền giá trị thiếu (imputation), vì các giá trị được điền có thể có kiểu dữ liệu khác (ví dụ: float sau khi điền bằng giá trị trung bình).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdff52d8-9af3-4f27-b014-eaffef0c9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast non-integer type to integer type\n",
    "\n",
    "# List of columns to cast\n",
    "columns_to_cast = [\n",
    "    \"ind_nuevo\",\n",
    "    \"indrel\",\n",
    "    \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\",\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\",\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\",\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\",\n",
    "    \"ind_nomina_ult1\",\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\",\n",
    "    \"renta\"\n",
    "]\n",
    "\n",
    "# Cast each column to IntegerType\n",
    "for column in columns_to_cast:\n",
    "    df = df.withColumn(column, df[column].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644968c",
   "metadata": {},
   "source": [
    "# Điền giá trị thiếu bằng giá trị trung vị (Median)\n",
    "\n",
    "Đoạn mã này thực hiện điền các giá trị thiếu (NA/null) của DataFrame PySpark (`df`) bằng giá trị *trung vị* của cột đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62a41-f4d6-4023-9f5d-058c56ef60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 14:52:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/23 14:52:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/23 14:52:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/23 14:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/23 14:52:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Fill with median value\n",
    "window_spec = Window.orderBy(F.col(\"fecha_alta\"))\n",
    "dates = df.select(\n",
    "    \"fecha_alta\",\n",
    "    F.row_number().over(window_spec).alias(\"index\")\n",
    ")\n",
    "total_rows = dates.count()\n",
    "median_index = (total_rows // 2) + 1 \n",
    "median_value = dates.filter(F.col(\"index\") == median_index).select(\"fecha_alta\").collect()[0][0]\n",
    "df = df.withColumn(\n",
    "    \"fecha_alta\",\n",
    "    F.when(F.col(\"fecha_alta\").isNull(), median_value).otherwise(F.col(\"fecha_alta\"))\n",
    "\n",
    "median_value = df.select(F.median(\"ind_actividad_cliente\")).collect()[0][0]\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"ind_actividad_cliente\",\n",
    "    F.when(F.col(\"ind_actividad_cliente\").isNull(), median_value).otherwise(F.col(\"ind_actividad_cliente\"))\n",
    ")\n",
    ")\n",
    "\n",
    "#Fill with median of province\n",
    "grouped = df.groupBy(\"nomprov\").agg(median(\"renta\").alias(\"renta_median\"))\n",
    "\n",
    "df = df.join(grouped, \"nomprov\", \"left\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"renta\",\n",
    "    F.when(F.col(\"renta\").isNull(), col(\"renta_median\")).otherwise(F.col(\"renta\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3c34a",
   "metadata": {},
   "source": [
    "# Loại bỏ các hàng chứa giá trị NULL trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện loại bỏ các hàng chứa giá trị NULL (hay NA - Not Available) trong các cột cụ thể của DataFrame (`df`). Việc loại bỏ hàng có giá trị NULL là một cách xử lý dữ liệu thiếu, thường được thực hiện khi số lượng giá trị thiếu không quá lớn và việc loại bỏ chúng không ảnh hưởng đáng kể đến kết quả phân tích."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081203e9-3f00-408c-a3a6-76e05dc42476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows that have NULL value\n",
    "# df = df.na.drop(subset=[\"nomprov\"]) \n",
    "df = df.na.drop(subset=[\"ind_nom_pens_ult1\"]) \n",
    "df = df.na.drop(subset=[\"ind_nomina_ult1\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df661f5c",
   "metadata": {},
   "source": [
    "# Ánh xạ giá trị chuỗi sang số nguyên (String to Integer Mapping) trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện chuyển đổi các giá trị chuỗi trong một số cột của DataFrame (`df`) thành các giá trị số nguyên tương ứng.  Việc này thường được thực hiện khi dữ liệu gốc chứa các giá trị dạng text (ví dụ: mã quốc gia, loại khách hàng) nhưng mô hình machine learning lại yêu cầu đầu vào là số.  Quá trình này được gọi là *label encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530c8b33-f888-46f2-86f6-22696b4be4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map a string value to a integer value\n",
    "canal_dict = {'KAI': 35,'KBG': 17,'KGU': 149,'KDE': 47,'KAJ': 41,'KCG': 59,\n",
    " 'KHM': 12,'KAL': 74,'KFH': 140,'KCT': 112,'KBJ': 133,'KBL': 88,'KHQ': 157,'KFB': 146,'KFV': 48,'KFC': 4,\n",
    " 'KCK': 52,'KAN': 110,'KES': 68,'KCB': 78,'KBS': 118,'KDP': 103,'KDD': 113,'KBX': 116,'KCM': 82,\n",
    " 'KAE': 30,'KAB': 28,'KFG': 27,'KDA': 63,'KBV': 100,'KBD': 109,'KBW': 114,'KGN': 11,\n",
    " 'KCP': 129,'KAK': 51,'KAR': 32,'KHK': 10,'KDS': 124,'KEY': 93,'KFU': 36,'KBY': 111,\n",
    " 'KEK': 145,'KCX': 120,'KDQ': 80,'K00': 50,'KCC': 29,'KCN': 81,'KDZ': 99,'KDR': 56,\n",
    " 'KBE': 119,'KFN': 42,'KEC': 66,'KDM': 130,'KBP': 121,'KAU': 142,'KDU': 79,\n",
    " 'KCH': 84,'KHF': 19,'KCR': 153,'KBH': 90,'KEA': 89,'KEM': 155,'KGY': 44,'KBM': 135,\n",
    " 'KEW': 98,'KDB': 117,'KHD': 2,'RED': 8,'KBN': 122,'KDY': 61,'KDI': 150,'KEU': 72,\n",
    " 'KCA': 73,'KAH': 31,'KAO': 94,'KAZ': 7,'004': 83,'KEJ': 95,'KBQ': 62,'KEZ': 108,\n",
    " 'KCI': 65,'KGW': 147,'KFJ': 33,'KCF': 105,'KFT': 92,'KED': 143,'KAT': 5,'KDL': 158,\n",
    " 'KFA': 3,'KCO': 104,'KEO': 96,'KBZ': 67,'KHA': 22,'KDX': 69,'KDO': 60,'KAF': 23,'KAW': 76,\n",
    " 'KAG': 26,'KAM': 107,'KEL': 125,'KEH': 15,'KAQ': 37,'KFD': 25,'KEQ': 138,'KEN': 137,\n",
    " 'KFS': 38,'KBB': 131,'KCE': 86,'KAP': 46,'KAC': 57,'KBO': 64,'KHR': 161,'KFF': 45,\n",
    " 'KEE': 152,'KHL': 0,'007': 71,'KDG': 126,'025': 159,'KGX': 24,'KEI': 97,'KBF': 102,\n",
    " 'KEG': 136,'KFP': 40,'KDF': 127,'KCJ': 156,'KFR': 144,'KDW': 132,-1: 6,'KAD': 16,\n",
    " 'KBU': 55,'KCU': 115,'KAA': 39,'KEF': 128,'KAY': 54,'KGC': 18,'KAV': 139,'KDN': 151,\n",
    " 'KCV': 106,'KCL': 53,'013': 49,'KDV': 91,'KFE': 148,'KCQ': 154,'KDH': 14,'KHN': 21,\n",
    " 'KDT': 58,'KBR': 101,'KEB': 123,'KAS': 70,'KCD': 85,'KFL': 34,'KCS': 77,'KHO': 13,\n",
    " 'KEV': 87,'KHE': 1,'KHC': 9,'KFK': 20,'KDC': 75,'KFM': 141,'KHP': 160,'KHS': 162,\n",
    " 'KFI': 134,'KGV': 43}\n",
    "\n",
    "\n",
    "pais_dict = {'LV': 102,'CA': 2,'GB': 9,'EC': 19,'BY': 64,'ML': 104,'MT': 118,\n",
    " 'LU': 59,'GR': 39,'NI': 33,'BZ': 113,'QA': 58,'DE': 10,'AU': 63,'IN': 31,\n",
    " 'GN': 98,'KE': 65,'HN': 22,'JM': 116,'SV': 53,'TH': 79,'IE': 5,'TN': 85,\n",
    " 'PH': 91,'ET': 54,'AR': 13,'KR': 87,'GA': 45,'FR': 8,'SG': 66,'LB': 81,\n",
    " 'MA': 38,'NZ': 93,'SK': 69,'CN': 28,'GI': 96,'PY': 51,'SA': 56,'PL': 30,\n",
    " 'PE': 20,'GE': 78,'HR': 67,'CD': 112,'MM': 94,'MR': 48,'NG': 83,'HU': 106,\n",
    " 'AO': 71,'NL': 7,'GM': 110,'DJ': 115,'ZA': 75,'OM': 100,'LT': 103,'MZ': 27,\n",
    " 'VE': 14,'EE': 52,'CF': 109,'CL': 4,'SL': 97,'DO': 11,'PT': 26,'ES': 0,\n",
    " 'CZ': 36,'AD': 35,'RO': 41,'TW': 29,'BA': 61,'IS': 107,'AT': 6,'ZW': 114,\n",
    " 'TR': 70,'CO': 21,'PK': 84,'SE': 24,'AL': 25,'CU': 72,'UY': 77,'EG': 74,'CR': 32,\n",
    " 'GQ': 73,'MK': 105,'KW': 92,'GT': 44,'CM': 55,'SN': 47,'KZ': 111,'DK': 76,\n",
    " 'LY': 108,'AE': 37,'PA': 60,'UA': 49,'GW': 99,'TG': 86,'MX': 16,'KH': 95,\n",
    " 'FI': 23,'NO': 46,'IT': 18,'GH': 88, 'JP': 82,'RU': 43,'PR': 40,'RS': 89,\n",
    " 'DZ': 80,'MD': 68,-1: 1,'BG': 50,'CI': 57,'IL': 42,'VN': 90,'CH': 3,'US': 15,'HK': 34,\n",
    " 'CG': 101,'BO': 62,'BR': 17,'BE': 12,'BM': 117}\n",
    "\n",
    "emp_dict = {'N':0,-1:-1,'A':1,'B':2,'F':3,'S':4}\n",
    "indfall_dict = {'N':0,-1:-1,'S':1}\n",
    "sexo_dict = {'V':0,'H':1,-1:-1}\n",
    "tiprel_dict = {'A':0,-1:-1,'I':1,'P':2,'N':3,'R':4}\n",
    "indresi_dict = {'N':0,-1:-1,'S':1}\n",
    "indext_dict = {'N':0,-1:-1,'S':1}\n",
    "conyuemp_dict = {'N':0,-1:-1,'S':1}\n",
    "segmento_dict = {-1:-1,'01 - TOP':1,'02 - PARTICULARES':2,'03 - UNIVERSITARIO':3}\n",
    "\n",
    "spark_df = df\n",
    "def create_mapping_udf(mapping_dict):\n",
    "    def map_value(value):\n",
    "        return mapping_dict.get(value, -1)  # Default to -1 if not found\n",
    "    return udf(map_value, IntegerType())\n",
    "\n",
    "canal_udf = create_mapping_udf(canal_dict)\n",
    "pais_udf = create_mapping_udf(pais_dict)\n",
    "indfall_udf = create_mapping_udf(indfall_dict)\n",
    "sexo_udf = create_mapping_udf(sexo_dict)\n",
    "tiprel_udf = create_mapping_udf(tiprel_dict)\n",
    "indresi_udf = create_mapping_udf(indresi_dict)\n",
    "indext_udf = create_mapping_udf(indext_dict)\n",
    "conyuemp_udf = create_mapping_udf(conyuemp_dict)\n",
    "segmento_udf = create_mapping_udf(segmento_dict)\n",
    "emp_udf = create_mapping_udf(emp_dict)\n",
    "\n",
    "def apply_udfs(spark_df):\n",
    "    spark_df = spark_df.withColumn(\"canal_entrada\", canal_udf(col(\"canal_entrada\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"pais_residencia\", pais_udf(col(\"pais_residencia\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"indfall\", indfall_udf(col(\"indfall\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"sexo\", sexo_udf(col(\"sexo\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"tiprel_1mes\", tiprel_udf(col(\"tiprel_1mes\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"indresi\", indresi_udf(col(\"indresi\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"indext\", indext_udf(col(\"indext\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"conyuemp\", conyuemp_udf(col(\"conyuemp\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"segmento\", segmento_udf(col(\"segmento\")).cast(IntegerType()))\n",
    "    spark_df = spark_df.withColumn(\"ind_empleado\", segmento_udf(col(\"ind_empleado\")).cast(IntegerType()))\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "spark_df = apply_udfs(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5c0f8",
   "metadata": {},
   "source": [
    "# Xử lý và tách cột ngày tháng (Date) trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện các thao tác sau trên các cột kiểu ngày tháng (Date) trong DataFrame (`spark_df`):\n",
    "\n",
    "1.  **Trích xuất thành phần ngày tháng:** Tách các thành phần năm, tháng, và ngày từ các cột ngày tháng gốc (`fecha_dato`, `ult_fec_cli_1t`, `fecha_alta`) và tạo các cột mới tương ứng.\n",
    "2.  **Xử lý giá trị NULL:** Điền giá trị `-1` vào bất kỳ giá trị NULL nào xuất hiện trong các cột mới tạo.\n",
    "3.  **Xóa cột gốc:** Loại bỏ các cột ngày tháng gốc sau khi đã trích xuất thông tin.\n",
    "4. **Tính tháng dạng số nguyên**: Tạo ra một cột tháng dạng số nguyên bằng cách lấy tháng cộng với 12 lần năm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bc651-962a-4763-ab69-0108d396aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three cells bellow format DATE type into month,day,year,*int and drop the DATE column\n",
    "spark_df = spark_df.withColumn(\"fecha_dato_month\", substring(\"fecha_dato\", 6, 2).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"fecha_dato_year\", (substring(\"fecha_dato\", 1, 4).cast(IntegerType()) - 2015))\n",
    "spark_df = spark_df.withColumn(\"month_int\", (col(\"fecha_dato_month\") + 12 * col(\"fecha_dato_year\")).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"fecha_dato_day\", substring(\"fecha_dato\", 9, 2).cast(IntegerType()))\n",
    "for col_name in [\"fecha_dato_month\", \"fecha_dato_year\", \"month_int\", \"fecha_dato_day\"]:\n",
    "    spark_df = spark_df.withColumn(col_name, \\\n",
    "                        when(col(col_name).isNull(), lit(-1)) \\\n",
    "                        .otherwise(col(col_name)))\n",
    "\n",
    "# Drop the original column\n",
    "spark_df = spark_df.drop(\"fecha_dato\")\n",
    "spark_df = spark_df.withColumn(\"ult_fec_cli_1t_month\", substring(\"ult_fec_cli_1t\", 6, 2).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"ult_fec_cli_1t_year\", (substring(\"ult_fec_cli_1t\", 1, 4).cast(IntegerType()) - 2015))\n",
    "spark_df = spark_df.withColumn(\"ult_fec_cli_1t_day\", substring(\"ult_fec_cli_1t\", 9, 2).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"ult_fec_cli_1t_month_int\", (col(\"ult_fec_cli_1t_month\") + 12 * col(\"ult_fec_cli_1t_year\")))\n",
    "    #Check if any value is null, then fill with -1\n",
    "for col_name in [\"ult_fec_cli_1t_month\", \"ult_fec_cli_1t_year\", \"ult_fec_cli_1t_day\", \"ult_fec_cli_1t_month_int\"]:\n",
    "    spark_df = spark_df.withColumn(col_name, \\\n",
    "                       when(col(col_name).isNull(), -1) \\\n",
    "                       .otherwise(col(col_name)))\n",
    "spark_df = spark_df.drop(\"ult_fec_cli_1t\")\n",
    "spark_df = spark_df.withColumn(\"fecha_alta_month\", substring(\"fecha_alta\", 6, 2).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"fecha_alta_year\", (substring(\"fecha_alta\", 1, 4).cast(IntegerType()) - 2015))\n",
    "spark_df = spark_df.withColumn(\"fecha_alta_day\", substring(\"fecha_alta\", 9, 2).cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"fecha_alta_month_int\", (col(\"fecha_alta_month\") + 12 * col(\"fecha_alta_year\")).cast(IntegerType()))\n",
    "\n",
    "# Drop the original column\n",
    "spark_df = spark_df.drop(\"fecha_alta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727058d",
   "metadata": {},
   "source": [
    "# Xử lý giá trị thiếu và chuyển đổi kiểu dữ liệu trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện một loạt các thao tác trên DataFrame (`spark_df`) để:\n",
    "\n",
    "1.  **Điền giá trị thiếu (NULL):** Thay thế các giá trị NULL bằng `-1` trong một số cột.\n",
    "2.  **Chuyển đổi kiểu dữ liệu:** Chuyển đổi kiểu dữ liệu của một số cột sang số nguyên (`IntegerType`).\n",
    "3.  **Thay thế giá trị:** Thay giá trị \"P\" bằng -2 trong cột `indrel_1mes`\n",
    "4.  **Xóa cột:** Loại bỏ cột `nomprov`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ad9cf-7183-4da3-993a-1157cdc5aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NULL value with -1 and cast to integer type\n",
    "spark_df = spark_df.withColumn(\"indrel_1mes\",when(col(\"indrel_1mes\") == \"P\", -2).otherwise(col(\"indrel_1mes\")))\n",
    "spark_df = spark_df.fillna({\"indrel_1mes\": -1})\n",
    "spark_df = spark_df.withColumn(\"ind_actividad_cliente\", when(col(\"ind_actividad_cliente\").isNull(), lit(-1)).otherwise(col(\"ind_actividad_cliente\").cast(IntegerType())))\n",
    "spark_df = spark_df.withColumn(\"indrel_1mes\", col(\"indrel_1mes\").cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"tipodom\", when(col(\"tipodom\").isNull(), lit(-1)).otherwise(col(\"tipodom\").cast(IntegerType())))\n",
    "spark_df = spark_df.withColumn(\"cod_prov\", when(col(\"cod_prov\").isNull(), lit(-1)).otherwise(col(\"cod_prov\").cast(IntegerType())))\n",
    "spark_df = spark_df.withColumn(\"antiguedad\",when(col(\"antiguedad\").isNull(), lit(-1)).otherwise(col(\"antiguedad\")))\n",
    "spark_df = spark_df.withColumn(\"antiguedad\", col(\"antiguedad\").cast(IntegerType()))\n",
    "spark_df = spark_df.drop(\"nomprov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2acb9-5651-4619-a18e-86363418fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of NULLs value in dataframe\n",
    "# Dict_Null = {col:spark_df.filter(spark_df[col].isNull()).count() for col in spark_df.columns}\n",
    "# Dict_Null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda380c",
   "metadata": {},
   "source": [
    "# Tạo độ trễ (Lag Features) trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này tạo các *đặc trưng trễ* (lag features) cho một tập hợp các cột trong DataFrame (`spark_df`).  Đặc trưng trễ là các giá trị của một cột ở các thời điểm *trước đó*.  Chúng thường rất hữu ích trong các bài toán dự đoán chuỗi thời gian (time series forecasting), nơi mà giá trị trong quá khứ có thể giúp dự đoán giá trị trong tương lai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55865aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lag to train the model\n",
    "target_cols = [ \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\",\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\",\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\",\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\",\n",
    "    \"ind_nomina_ult1\",\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\"]\n",
    "\n",
    "w = Window.partitionBy(\"ncodpers\").orderBy(\"month_int\")\n",
    "\n",
    "lag_months =[1,2,3,4,5,6]\n",
    "for lag in lag_months:\n",
    "    for col in target_cols:\n",
    "        spark_df = spark_df.withColumn(f\"lag_{lag}_{col}\", F.lag(F.col(col),lag).over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae787e8",
   "metadata": {},
   "source": [
    "# Tạo độ trễ (Lag Features) trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này tạo các *đặc trưng trễ* (lag features) cho một tập hợp các cột trong DataFrame (`spark_df`).  Đặc trưng trễ là các giá trị của một cột ở các thời điểm *trước đó*.  Chúng thường rất hữu ích trong các bài toán dự đoán chuỗi thời gian (time series forecasting), nơi mà giá trị trong quá khứ có thể giúp dự đoán giá trị trong tương lai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window partitioned by customer and ordered by date\n",
    "w = Window.partitionBy(\"ncodpers\").orderBy(\"month_int\")\n",
    "\n",
    "# Compute the previous month value to calculate the gap\n",
    "spark_df = spark_df.withColumn(\"prev_month\", F.lag(\"month_int\").over(w))\n",
    "spark_df = spark_df.withColumn(\"month_diff\", \n",
    "                   F.when(F.col(\"prev_month\").isNotNull(), \n",
    "                          F.col(\"month_int\") - F.col(\"prev_month\"))\n",
    "                    .otherwise(0))\n",
    "\n",
    "# Flag a gap if the difference is not equal to 1 (and ignore the first record)\n",
    "spark_df = spark_df.withColumn(\"gap_flag\", \n",
    "                   F.when((F.col(\"month_diff\") != 1) & F.col(\"prev_month\").isNotNull(), 1)\n",
    "                    .otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949e8ce",
   "metadata": {},
   "source": [
    "# Xuất dữ liệu đã làm sạch sang file Parquet trong PySpark\n",
    "\n",
    "Đoạn mã PySpark này thực hiện việc lưu trữ DataFrame (`spark_df`) đã được làm sạch và xử lý vào một file Parquet.  Parquet là một định dạng file cột (columnar storage format) phổ biến, được tối ưu hóa cho các hệ thống phân tích dữ liệu lớn như Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19916e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export cleaned data to .parquet file to train train the model\n",
    "# spark_df.printSchema()\n",
    "spark_df.write.mode(\"overwrite\").parquet(\"train_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
